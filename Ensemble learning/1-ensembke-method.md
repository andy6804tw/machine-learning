# Ensemble learning
Ensemble learning試圖以一個系統化的方式將好幾個監督式學習的模型結合在一起，目的是希望結合眾多的模型產生一個更強大的模型。在許多科學競賽中Ensemble learning在實務上是非常有效的提升預測準確率。

為什麼Ensemble learning強大？假如我們有好幾個彼此獨立而且歧異性夠高的模型，這些模型我們稱為base learner。他們統合之後的結果通常比較精準，原因是因為每一個base learner有各自的偏見但是也有各自合理的地方。合理的地方會互相加強，各自偏見的地方會互相抵銷掉。

更具體的舉例來說假設我們有三個獨立且歧異性夠高的二元分類得演算法，每一個準確率都0.6。假設我們想將這三個模型整合在一起，整合的方式是三個裡面果有兩個以上都認為是某個類別，最後預測的結果就是該類別。此種方法是採多數決的方式。

依照Ensemble的方式，我們可以分為三類。第一類為Bagging，第二類為Boosting，第三類為Stacking。Bagging指的是我們把訓練資料重新採樣采聲不同組的訓練資料，根據不同組的訓練資料即使我們用同一種演算法我們也會得到不一樣的模型。Boosting則會根據每一筆訓練資料的難或簡單給予不同的權重，首先我們會訓練一個base learner然後根據base learner預測的結果對或錯來分辨該筆資料是一個簡單還是困難的資料。對於難的資料我們加強他的權重在訓練一個新的分類器或回歸器，我們目標是希望再訓練後新的模型在這些難的資料能夠夠表現得更好。我們不斷重複這些步驟，不斷地加入新的base learner，且新的base learner把過去表現不好的地方改善，這就是Boosting精神。最後一個是Stacking，假設我們有好幾個base learner，每一個base learner都有判斷解決問題的能力。Stacking會把這些base learner的輸出當成是一個新的模型的輸入，最後再重新訓練一個模型來預測最終結果。

### 1. Bagging (resample training data)
Bagging 是 Bootstrap aggregating 的縮寫，Bootstrap 指的是假設我們有n筆資料我們從中抽取n’筆資料出來，這n’筆資料是可以被重複抽取的。假設我們有一萬筆資料我們要從中抽取100筆資料出來，這100筆資料裡面可能會有重複的數據，這意味著同一筆資料可能會重複抽到好幾次，這種概念即為 Bootstrap。Bootstrap aggregating 就是重複 Bootstrap 步驟重複 m 次，做完之後我們會有 m 組的訓練資料，每一組訓練資料內都有 n’ 筆資料。最後我們會根據這 m 組的訓練資料，每一組都會去訓練出一個模型，就算我們使用同一個訓練的演算法，因為訓練資料不同因此我們會得到 m 個不同的模型。這 m 個模型每一個都是微弱的learner，最後合併在一起就會形成一個比較強的learner了
- Random forest
隨機森林其實就是進階版的決策樹，這句話簡單說明，就是很多顆樹加起來可以變成一座森林。隨機森林是使用 Bagging + randomizes freature set 的技術所產生出來的 Ensemble learning 演算法。隨機森林就是由很多的樹集合在一起，每一棵樹都是各自的決策樹。然而每一棵樹的訓練資料是經由 Bagging + randomizes freature set 所產生的。所謂的 Bagging 是每一棵樹的從原始資料中(random sample with replacement)取用放回的方式隨機的產生訓練資料。除此之外我們要求每一棵樹只能看見部分的特徵，換句話說假設我們有一萬筆資料，每筆資料有100個特徵，每一棵樹只能從這一萬筆資料透過 Bagging 的方式隨機拿出n筆資料。除此之外這n筆資料可能只隨機挑選k個特徵做樣本。因此在隨機森林每一棵樹的特徵數量可能都不同，所以最後決策出來的結果都會不一樣。最後再根據任務的不同來做回歸或是分類的問題，如果是回歸問題我們就將這些決策數的輸出做平均得到最後答案，若是分類問題我們則用投標採多數決的方式來整合所有樹預測的結果。

每一棵樹在生成的過程中，可能用到不同的訓練資料和不一樣數量的特徵。會用到哪些訓練資料及特徵都是由隨機決定。

因為我們要訓練很多樹，因此不管在訓練或是預測的時候都會要更多的運算資源，不過因為在隨機森林每一棵樹都是獨立的，所以不管是在訓練或是預測的階段每一棵樹都能平行化的運行。

![](https://i.imgur.com/v2Sm3rB.png)

#### Bagging 小結
Bagging 是在訓練資料上動手腳，即使我們用同一個監督式學習的演算法，會因為訓練集的不同產生不一樣的模型。那如何產生不一樣的訓練資料？我們是用 Bagging 的方式 random sample with replacement 產生好幾組不一樣的訓練資料。每一組訓練資料會產生出一個模型，最後再將好幾個決策樹的模型整合在一起得到最終的預測。


### 2. Boosting (reweight training data + weighted models)
相較於 Bagging 而 Boosting 有下面幾個特色。第一個是每個的模型重要程度是不一樣的，當我們要整合最後的模型時每個模型的權重是不一樣的。第二個在訓練的時候每一筆的資料權重也是不一樣的，有些資料可能在某一迭代的權重是比較高的，而在其他迭代中該筆資料權重可能是比較低的。

![](https://i.imgur.com/kZPcFvJ.png)

- AdaBoost
AdaBoost 是 Adaptive Boosting 的縮寫。需要人工的給定 hyperparameter (K)，也就是希望能產生出K個 base learners。當某筆資料在這一回合被猜錯了，代表下一回合他的權重比較大會較重要，想辦法讓機器學會。這邊有兩個權重第一個為模型權重代表他預測有多準(a)，另一個是筆一筆訓練資料他在這一回合有多重要(w)。

```
yi=a1f1(xi)+a2f2(xi)+...+akfk(xi)

wi代表某筆資料在該回合的重要程度，若猜錯wi重要程度會比較大代表下一回合 要讓他學會
a代表此模型的權重 err越大a越小代表此模型參考價值不高
```

- Gradient Boosting

3. Stacking
- blending weak learners

